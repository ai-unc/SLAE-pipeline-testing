import json
import yaml
from openai import OpenAI
from langchain.pydantic_v1 import BaseModel, validator, Field
from typing import List,Dict
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import pytorch_cos_sim
from sklearn.cluster import KMeans
from langchain.output_parsers import PydanticOutputParser
from random import shuffle
from statistics import mean, median, stdev
from os import listdir, getenv
from datetime import datetime

key = getenv("OPENAI_API_KEY")
client = OpenAI(api_key=key)

class SingleRelation(BaseModel):
    independent_variable_name: str
    dependent_variable_name: str
    is_causal: str
    attributes: str
    supporting_text: str
    relation_classification: str
    
    @validator("relation_classification")
    def allowed_classifications(cls, field):
        if field.lower() in {"direct", "inverse", "not applicable", "independent"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

class ListOfRelations(BaseModel):
  relations: list[SingleRelation]

class RelationCountError(Exception):
  def __init__(self, message):
    self.message = message
    super().__init__(self.message)
    super().__init__(self.message)

#Define additional functions for pipeline

def clean_data(data_path:str) -> dict:
    """Reads Json and removes list of user predictions"""
    with open(data_path, "r") as f:
        data = json.load(f)
    for relation in data['Relations']:
        relation["RelationshipClassification"] = ""
        relation["isCausal"] = ""
        relation["SupportingText"] = ""
    return data  

def extract_all_ordered_pairs(data:Dict) -> List[str]:
    #Extract the relationships
    relationships = data.get("Relations", [])
    variable_pairs = []
    # Iterate through each relationship and extract variables
    for relationship in relationships:
        variable_one = relationship.get("VariableOneName", "")
        variable_two = relationship.get("VariableTwoName", "")
        variable_pairs.append(variable_one + " -> " + variable_two)
    return variable_pairs

def call_LLM(instruction, text, model:str) -> str:
  """Used to call a OpenAI LLM model"""
  print("------------------------------------START PROMPT------------------------------------")
  print(text)
  print("-------------------------------------END PROMPT-------------------------------------")
  completion = client.chat.completions.create(
    model=model,
    messages=[
        {"role": "system", "content": instruction},
        {"role": "user", "content": text}
    ]   
  )
  return completion.choices[0].message.content
    
def summarize(text:str, model:str) -> str:
  """Used to summarize text during pre-processing
  Currently unused in OpenAI pipeline
  """
  prompt = \
    f"""
    You will be given exceprts of an academic paper. Your task is to summarize the main points of the paper.
    Include a comprehensive overview of the main research question, methodology, key findings, and conclusions.
    Emphasize the findings by detailing the data analysis methods used,
    significant results, how these results address the research question,
    and any implications or recommendations made by the authors.
    Also, mention any limitations of the study acknowledged by the authors.
    Conclude with the potential impact of this research in its respective field.
    Write your response in the form of one or multiple paragraphs.
    """
  return call_LLM(prompt, text, model)

def pipeline(data:Dict, instructions:str, text_format:str, *, debug:bool=False) -> Dict:
  """
  data should already be cleaned
  """
  paper_text = data["PaperContents"]
  
  
  # Summarize paper
  if debug: print("Summarizing paper...")
  
  paper_text = summarize(paper_text, model)  
  
  if debug: print(f"Summarized text:\n{paper_text}")
    
  # Extract relationships from the summarized text
  relationships = extract_all_ordered_pairs(data)
  parser = PydanticOutputParser(pydantic_object=ListOfRelations) #Refers to a class called SingleRelation.
  parsed_output = {}
  BATCH_SIZE = 5
  for i in range(0, len(relationships), BATCH_SIZE):
    input_text = prompt_template.format_prompt(text=paper_text, relationships="\n".join(relationships[i:i+BATCH_SIZE]), count=len(relationships[i:i+BATCH_SIZE])).to_string()
    print(input_text)
    batch_output = call_LLM(input_text, model)
    parsed_batch_output = parser.parse(batch_output)
    if debug: print(f"Batch {i//BATCH_SIZE + 1} complete. Output:\n{parsed_batch_output.dict()}")
    if "Relations" not in parsed_output:
      parsed_output["Relations"] = parsed_batch_output.dict()["Relations"]
    else:
      parsed_output["Relations"] += parsed_batch_output.dict()["Relations"]
  
  if debug: print(f"Pipeline complete. Output:\n{[r['RelationshipClassification'] for r in parsed_output['Relations']]}")
  
  # ensure only desired relations are present

  final_output = {"Relations":data["Relations"]}
  #set all relations to a default of not applicable
  for relation in final_output["Relations"]:
    relation["RelationshipClassification"] = "not applicable"
  
  # Create a dictionary for quick lookup of relations
  parsed_relations_dict = {
    (relation["VariableOneName"], relation["VariableTwoName"]): relation
    for relation in parsed_output["Relations"]
}

  # Update the final_output based on the dictionary
  for relation in final_output["Relations"]:
    key = (relation["VariableOneName"], relation["VariableTwoName"])
    if key in parsed_relations_dict:
        parsed_relation = parsed_relations_dict[key]
        relation["RelationshipClassification"] = parsed_relation["RelationshipClassification"]
        relation["isCausal"] = parsed_relation["isCausal"]
        relation["SupportingText"] = parsed_relation["SupportingText"]
  
  return final_output

def call_pipeline(data_path:str, settings_path:str, *, use_alternate_names=False) -> Dict:
  with open(settings_path, "r") as f:
    pipeline_settings = yaml.safe_load(f)
    debug = pipeline_settings["verbose"]
    instructions = pipeline_settings["instructions"]
    text = pipeline_settings["prompt"]
    model = pipeline_settings["model"]
  data = clean_data(data_path)
  if use_alternate_names:
    for relation in data["relations"]:
      relation["independent_variable_name"] = relation["alternate_independent_variable_name"]
      relation["dependent_variable_name"] = relation["alternate_dependent_variable_name"]
  return pipeline(data, instructions, text, model, debug=debug)

if __name__ == "__main__":
  EVALUATE = True
  RANDOMIZE = False
  DEBUG = True
  USE_ALTERNATE_NAMES = False
  NUM_TRIALS = 1
  NUM_PAPERS = 1
  
  def score(solution:List[Dict], submission:List[Dict]) -> List[float]:
    scores = {}
    for i, paper in enumerate(submission):
      ground_truth = solution[i]
      if DEBUG:
        print("\n\n\nGUESS:")
        print(*[r["relation_classification"] for r in paper["relations"]], sep="\n")
        print("\n\n\nGROUND TRUTH:")
        print(*[r["relation_classification"] for r in ground_truth["relations"]], sep="\n")
        
      if "relations" not in paper or "relations" not in ground_truth:
        raise Exception("Key error: relations.")
      if len(paper["relations"]) != len(ground_truth["relations"]):
        print("\n\n\nGUESS:")
        print(*extract_all_ordered_pairs(paper), sep="\n")
        print("\n\n\nGROUND TRUTH:")
        print(*extract_all_ordered_pairs(ground_truth), sep="\n")
        raise RelationCountError(f"Prediction has {len(paper['relations'])} relations and ground truth has {len(ground_truth['relations'])}.")
      
      score = 0
      for j, prediction in enumerate(paper["relations"]):
        relation = ground_truth["relations"][j]
        score += 1 if relation["relation_classification"].lower().strip() == prediction["relation_classification"].lower().strip() else 0
      paper_score = (score / len(ground_truth["relations"])) * 100
      scores[ground_truth["title"]] = paper_score
    
    return scores
  
  if EVALUATE:
    trial_scores = []
      
    for _ in range(NUM_TRIALS):
        # Load data
        source = [file for file in listdir("./pipeline_evaluator/full_dataset")]
        if RANDOMIZE:
          shuffle(source)
        
        # Make Predictions
        predictions = []
        for paper in source[:NUM_PAPERS]:
            print(f"Predicting {paper}")
            prediction = call_pipeline(data_path=f"./pipeline_evaluator/full_dataset/{paper}",
                                        settings_path="./pipelines/clustered_relations_pipeline/pipeline_settings_openai.yaml",
                                        use_alternate_names=USE_ALTERNATE_NAMES)
            predictions.append(prediction)
        
        #score
        papers = []
        for p in source[:NUM_PAPERS]:
            with open(f"./pipeline_evaluator/full_dataset/{p}") as f:
                papers.append(json.load(f))
        eval_scores = score(papers, predictions)
        trial_scores.append(mean(eval_scores.values()))
    print("\n\n\n")
    if len(trial_scores) == 1:
      for title, score_ in eval_scores.items():
        print(f"{title}: {score_}")
      if len(eval_scores) > 1:
        print(f"Average accuracy score: {mean(eval_scores.values())}")
        print(f"Median accuracy score: {median(eval_scores.values())}")
        print(f"Standard deviation: {stdev(eval_scores.values())}")
    else:
      print("Number of trials:", NUM_TRIALS)
      print(f"Accuracy scores: {trial_scores}")
      print(f"Average accuracy score: {mean(trial_scores)}")
      print(f"Median accuracy score: {median(trial_scores)}")
      print(f"Standard deviation: {stdev(trial_scores)}")
    with open(f"./pipeline_evaluator/results/results.txt", "a") as f:
      f.write(f"{__file__.rpartition('/')[-1]}-{datetime.now().strftime('%m/%d/%Y-%H:%M:%S')} : {eval_scores}\n")